{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-25 02:03:17 __init__.py:190] Automatically detected platform cuda.\n",
      "INFO 04-25 02:03:24 config.py:542] This model supports multiple tasks: {'embed', 'classify', 'reward', 'generate', 'score'}. Defaulting to 'generate'.\n",
      "INFO 04-25 02:03:24 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='results/pararel_fix', speculative_config=None, tokenizer='results/pararel_fix', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=results/pararel_fix, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 04-25 02:03:26 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 04-25 02:03:26 model_runner.py:1110] Starting to load model results/pararel_fix...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2acb46907124014b448f04cd640bbdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-25 02:03:27 model_runner.py:1115] Loading model weights took 2.8875 GB\n",
      "INFO 04-25 02:03:29 worker.py:267] Memory profiling takes 1.17 seconds\n",
      "INFO 04-25 02:03:29 worker.py:267] the current vLLM instance can use total_gpu_memory (47.29GiB) x gpu_memory_utilization (0.90) = 42.56GiB\n",
      "INFO 04-25 02:03:29 worker.py:267] model weights take 2.89GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 2.02GiB; the rest of the memory reserved for KV Cache is 37.57GiB.\n",
      "INFO 04-25 02:03:29 executor_base.py:110] # CUDA blocks: 87931, # CPU blocks: 9362\n",
      "INFO 04-25 02:03:29 executor_base.py:115] Maximum concurrency for 32768 tokens per request: 42.94x\n",
      "INFO 04-25 02:03:31 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:15<00:00,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-25 02:03:47 model_runner.py:1562] Graph capturing finished in 16 secs, took 0.22 GiB\n",
      "INFO 04-25 02:03:47 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 19.68 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "from argparse import ArgumentParser\n",
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "import ipdb\n",
    "from vllm import LLM, SamplingParams\n",
    "import json\n",
    "import os\n",
    "model_name = \"results/pararel_fix\"#\"/data/ah/code/rl/open-rs/results/pararel\"\n",
    "# cuda :1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "model = LLM(\n",
    "    model=model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.open_r1.rewards_u import extract_content\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "# 定义采样参数\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.1,\n",
    "    max_tokens=30\n",
    ")\n",
    "\n",
    "data_path = \"dataset/ID_test_pararel.json\"\n",
    "# data_path = \"dataset/OOD_test_pararel.json\"\n",
    "# data_path = \"dataset/pararel_training.json\"\n",
    "dataset = json.load(open(data_path))\n",
    "\n",
    "def inference(input_text):\n",
    "    outputs = model.generate(\n",
    "            input_text,\n",
    "            sampling_params=sampling_params\n",
    "    )\n",
    "    \n",
    "    output_text = outputs[0].outputs[0].text\n",
    "    # 找到第一个</confidence>， 保留第一个</confidence>及其之前的文本\n",
    "    result = output_text.split('</confidence>')[0] + '</confidence>'\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "sys_prompt = \"\"\"\n",
    "  You are a faithful assistant. The user asks a question, and you provide the answer and your confidence expressed in sure or unsure.\n",
    "  Question: Who is representing The Stranglers? \n",
    "  Answer: <answer>EMI</answer>\n",
    "  <confidence>sure</confidence>\n",
    "\n",
    "  Question: Who is the president of the Shao Qiao? \n",
    "  Answer: <answer>Yucheng Yang</answer>\n",
    "  <confidence>unsure</confidence>\n",
    "  \n",
    "  Question: Who produced the Japanese battleship Mikasa? \n",
    "  Answer: <answer>Vickers</answer>\n",
    "  <confidence>sure</confidence>\n",
    "  \n",
    "  Question: What is MaYj asap's twin city? \n",
    "  Answer: <answer>Beijing</answer>\n",
    "  <confidence>unsure</confidence>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "rft_pararel_results = []\n",
    "total_sample = len(dataset)\n",
    "correct_sample = 0\n",
    "un_c_match_sample = 0\n",
    "for data in tqdm(dataset):\n",
    "    question = data[\"question\"]\n",
    "    answer = data[\"answer\"]\n",
    "    prompt = sys_prompt + \"Question: \" + question +\"\\n\"+ \" Answer: \"\n",
    "    # input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda:1\")\n",
    "    # outputs = model.generate(input_ids, max_new_tokens=100)\n",
    "    # response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # print(response)\n",
    "    # print('-'*100)\n",
    "    output_text = inference(prompt)\n",
    "    model_ans = extract_content(output_text, \"answer\")\n",
    "    model_confidence = extract_content(output_text, \"confidence\")\n",
    "    # lines = output_text.split('\\n')\n",
    "    # if len(lines) == 0:\n",
    "    #     lines.append(\"\")\n",
    "    # if len(lines) == 1:\n",
    "    #     lines.append(\"\")\n",
    "    # model_ans = lines[0]\n",
    "    # model_confidence = lines[1]\n",
    "    correct = 1 if answer.lower() in model_ans.lower() else 0\n",
    "    un_c_match = 1 if (\"unsure\" in model_confidence.lower() and correct == 0 ) or (\"sure\" in model_confidence.lower() and correct == 1) else 0\n",
    "    # ipdb.set_trace()\n",
    "    rft_pararel_results.append({\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"model_ans\": model_ans,\n",
    "        \"model_confidence\": model_confidence,\n",
    "        \"correct\": correct,\n",
    "        \"un_c_match\": un_c_match,\n",
    "        \"output_text\": output_text\n",
    "    })\n",
    "    correct_sample += correct\n",
    "    un_c_match_sample += un_c_match\n",
    "\n",
    "print(f\"total_sample: {total_sample}\")\n",
    "print(f\"correct_sample: {correct_sample}\")\n",
    "print(f\"un_c_match_sample: {un_c_match_sample}\")\n",
    "print(f\"accuracy: {correct_sample/total_sample}\")\n",
    "print(f\"un_c_match_accuracy: {un_c_match_sample/total_sample}\")\n",
    "\n",
    "rft_pararel_results.append({\n",
    "    \"total_sample\": total_sample,\n",
    "    \"correct_sample\": correct_sample,\n",
    "    \"accuracy\": correct_sample/total_sample,\n",
    "    \"un_c_match_sample\": un_c_match_sample,\n",
    "    \"un_c_match_accuracy\": un_c_match_sample/total_sample,\n",
    "})\n",
    "# save results\n",
    "os.makedirs(\"results/eval_pararel/fixed_u\", exist_ok=True)\n",
    "json.dump(rft_pararel_results, open(\"results/eval_pararel/fixed_u/ID_rft_pararel_results_format.json\", \"w\"), indent=4)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  5.52it/s, est. speed input: 980.79 toks/s, output: 167.17 toks/s]\n",
      "100%|█████████▉| 5579/5584 [16:57<00:00,  6.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <answer>EMI</answer>\n",
      " <confidence>sure</confidence>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  5.49it/s, est. speed input: 964.56 toks/s, output: 166.29 toks/s]\n",
      "100%|█████████▉| 5580/5584 [16:57<00:00,  6.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <answer>Sandra Tilley</answer>\n",
      " <confidence>sure</confidence>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  5.51it/s, est. speed input: 962.27 toks/s, output: 166.86 toks/s]\n",
      "100%|█████████▉| 5581/5584 [16:57<00:00,  5.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <answer>Jimmy Mack</answer>\n",
      " <confidence>sure</confidence>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  5.62it/s, est. speed input: 984.18 toks/s, output: 170.65 toks/s]\n",
      "100%|█████████▉| 5582/5584 [16:57<00:00,  5.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <answer>Honey Chile</answer>\n",
      " <confidence>sure</confidence>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  5.46it/s, est. speed input: 955.43 toks/s, output: 165.67 toks/s]\n",
      "100%|█████████▉| 5583/5584 [16:58<00:00,  5.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <answer>High Energy</answer>\n",
      " <confidence>sure</confidence>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  5.53it/s, est. speed input: 976.40 toks/s, output: 167.37 toks/s]\n",
      "100%|██████████| 5584/5584 [16:58<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <answer>Breaks Co-Op</answer>\n",
      " <confidence>sure</confidence>\n",
      "total_sample: 5584\n",
      "correct_sample: 1836\n",
      "un_c_match_sample: 1994\n",
      "accuracy: 0.32879656160458454\n",
      "un_c_match_accuracy: 0.3570916905444126\n"
     ]
    }
   ],
   "source": [
    "from src.open_r1.rewards_u import extract_content\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "# 定义采样参数\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.1,\n",
    "    max_tokens=30\n",
    ")\n",
    "\n",
    "data_path = \"dataset/ID_test_pararel.json\"\n",
    "# data_path = \"dataset/OOD_test_pararel.json\"\n",
    "# data_path = \"dataset/pararel_training.json\"\n",
    "dataset = json.load(open(data_path))\n",
    "\n",
    "def inference(input_text):\n",
    "    outputs = model.generate(\n",
    "            input_text,\n",
    "            sampling_params=sampling_params\n",
    "    )\n",
    "    \n",
    "    output_text = outputs[0].outputs[0].text\n",
    "    # 找到第一个</confidence>， 保留第一个</confidence>及其之前的文本\n",
    "    result = output_text.split('</confidence>')[0] + '</confidence>'\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "sys_prompt = \"\"\"\n",
    "  You are a faithful assistant. The user asks a question, and you provide the answer and your confidence expressed in sure or unsure.\n",
    "  Question: Who is representing The Stranglers? \n",
    "  Answer: <answer>EMI</answer>\n",
    "  <confidence>sure</confidence>\n",
    "\n",
    "  Question: Who is the president of the Shao Qiao? \n",
    "  Answer: <answer>Yucheng Yang</answer>\n",
    "  <confidence>unsure</confidence>\n",
    "  \n",
    "  Question: Who produced the Japanese battleship Mikasa? \n",
    "  Answer: <answer>Vickers</answer>\n",
    "  <confidence>sure</confidence>\n",
    "  \n",
    "  Question: What is MaYj asap's twin city? \n",
    "  Answer: <answer>Beijing</answer>\n",
    "  <confidence>unsure</confidence>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "rft_pararel_results = []\n",
    "total_sample = len(dataset)\n",
    "correct_sample = 0\n",
    "un_c_match_sample = 0\n",
    "for data in tqdm(dataset):\n",
    "    question = data[\"question\"]\n",
    "    answer = data[\"answer\"]\n",
    "    prompt = sys_prompt + \"Question: \" + question +\"\\n\"+ \" Answer: \"\n",
    "    # input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda:1\")\n",
    "    # outputs = model.generate(input_ids, max_new_tokens=100)\n",
    "    # response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # print(response)\n",
    "    # print('-'*100)\n",
    "    output_text = inference(prompt)\n",
    "    # model_ans = extract_content(output_text, \"answer\")\n",
    "    # model_confidence = extract_content(output_text, \"confidence\")\n",
    "    lines = output_text.split('\\n')\n",
    "    if len(lines) == 0:\n",
    "        lines.append(\"\")\n",
    "    if len(lines) == 1:\n",
    "        lines.append(\"\")\n",
    "    model_ans = lines[0]\n",
    "    model_confidence = lines[1]\n",
    "    correct = 1 if answer.lower() in model_ans.lower() else 0\n",
    "    un_c_match = 1 if (\"unsure\" in model_confidence.lower() and correct == 0 ) or (\"sure\" in model_confidence.lower() and correct == 1) else 0\n",
    "    # ipdb.set_trace()\n",
    "    rft_pararel_results.append({\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"model_ans\": model_ans,\n",
    "        \"model_confidence\": model_confidence,\n",
    "        \"correct\": correct,\n",
    "        \"un_c_match\": un_c_match,\n",
    "        \"output_text\": output_text\n",
    "    })\n",
    "    correct_sample += correct\n",
    "    un_c_match_sample += un_c_match\n",
    "\n",
    "print(f\"total_sample: {total_sample}\")\n",
    "print(f\"correct_sample: {correct_sample}\")\n",
    "print(f\"un_c_match_sample: {un_c_match_sample}\")\n",
    "print(f\"accuracy: {correct_sample/total_sample}\")\n",
    "print(f\"un_c_match_accuracy: {un_c_match_sample/total_sample}\")\n",
    "\n",
    "rft_pararel_results.append({\n",
    "    \"total_sample\": total_sample,\n",
    "    \"correct_sample\": correct_sample,\n",
    "    \"accuracy\": correct_sample/total_sample,\n",
    "    \"un_c_match_sample\": un_c_match_sample,\n",
    "    \"un_c_match_accuracy\": un_c_match_sample/total_sample,\n",
    "})\n",
    "# save results\n",
    "os.makedirs(\"results/eval_pararel/fixed_u\", exist_ok=True)\n",
    "json.dump(rft_pararel_results, open(\"results/eval_pararel/fixed_u/ID_rft_pararel_results_line.json\", \"w\"), indent=4)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data_path = \"dataset/ID_test_pararel.json\"\n",
    "data_path = \"dataset/pararel_training.json\"\n",
    "dataset = json.load(open(data_path))\n",
    "  # You are a faithful assistant. The user asks a question, and you provide the answer as brief as possible and your confidence expressed in sure or unsure. \n",
    "\n",
    "sys_prompt = \"\"\"\n",
    "  You are a faithful assistant. The user asks a question, and you provide the answer and your confidence expressed in sure or unsure.\n",
    "  Question: Who is representing The Stranglers? \n",
    "  Answer: <answer>EMI</answer>\n",
    "  <confidence>sure</confidence>\n",
    "\n",
    "  Question: Who is the president of the Shao Qiao? \n",
    "  Answer: <answer>Yucheng Yang</answer>\n",
    "  <confidence>unsure</confidence>\n",
    "  \n",
    "  Question: Who produced the Japanese battleship Mikasa? \n",
    "  Answer: <answer>Vickers</answer>\n",
    "  <confidence>sure</confidence>\n",
    "  \n",
    "  Question: What is MaYj asap's twin city? \n",
    "  Answer: <answer>Beijing</answer>\n",
    "  <confidence>unsure</confidence>\n",
    "\n",
    "\"\"\"\n",
    "for data in dataset[:10]:\n",
    "    question = data[\"question\"]\n",
    "    answer = data[\"answer\"]\n",
    "    prompt = sys_prompt + \"Question: \" + question +\"\\n\"+ \" Answer: \"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda:1\")\n",
    "    outputs = model.generate(input_ids, max_new_tokens=100)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(response)\n",
    "    print('-'*100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompts = [\"<answer>\",\"<\", \"answer\", \">\", \"</answer>\", \"</\"]\n",
    "for prompt in prompts:\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    print(input_ids)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_content(text, tag_type=\"answer\", model_type=\"default\"):\n",
    "    \"\"\"Extract content based on tag type and model type.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to extract content from\n",
    "        tag_type (str): Type of content to extract (\"answer\" or \"confidence\")\n",
    "        model_type (str): Type of model format (\"gemma\" or \"default\")\n",
    "    \n",
    "    Returns:\n",
    "        str: Extracted content\n",
    "    \"\"\"\n",
    "    \n",
    "    if tag_type == \"answer\":\n",
    "        pattern = r\"<answer>(.*?)</answer>\"\n",
    "    else:  # confidence\n",
    "        pattern = r\"<confidence>(.*?)</confidence>\"\n",
    "\n",
    "\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return matches[0].strip() if matches else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"<answer>  AD as kk usl  </answer><confidence>    a</confidence>\"\n",
    "answer = extract_content(a, \"answer\").lower().split()\n",
    "confidence = extract_content(a, \"confidence\").lower().split()\n",
    "print(answer)\n",
    "print(confidence)\n",
    "print('a' in answer)\n",
    "print('a' in confidence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_set_ids = [0, 1, 1, 2, 2, 3, 3, 4, 4, 4, 4, 4, 4]\n",
    "semantic_set_counts = [semantic_set_ids.count(semantic_set_ids[i]) for i in range(len(semantic_set_ids))]\n",
    "print(semantic_set_counts)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
