{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-07 02:16:40 __init__.py:190] Automatically detected platform cuda.\n",
      "INFO 05-07 02:16:48 config.py:542] This model supports multiple tasks: {'classify', 'generate', 'embed', 'score', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 05-07 02:16:48 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='results/qwen/pararel_batch10_strict_answer_origin_data_0.7temp', speculative_config=None, tokenizer='results/qwen/pararel_batch10_strict_answer_origin_data_0.7temp', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=results/qwen/pararel_batch10_strict_answer_origin_data_0.7temp, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 05-07 02:16:50 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 05-07 02:16:50 model_runner.py:1110] Starting to load model results/qwen/pararel_batch10_strict_answer_origin_data_0.7temp...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d51c8c7f18435b99f96f3e397b40ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-07 02:16:51 model_runner.py:1115] Loading model weights took 2.8875 GB\n",
      "INFO 05-07 02:16:52 worker.py:267] Memory profiling takes 1.16 seconds\n",
      "INFO 05-07 02:16:52 worker.py:267] the current vLLM instance can use total_gpu_memory (47.29GiB) x gpu_memory_utilization (0.90) = 42.56GiB\n",
      "INFO 05-07 02:16:52 worker.py:267] model weights take 2.89GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 2.02GiB; the rest of the memory reserved for KV Cache is 37.57GiB.\n",
      "INFO 05-07 02:16:53 executor_base.py:110] # CUDA blocks: 87931, # CPU blocks: 9362\n",
      "INFO 05-07 02:16:53 executor_base.py:115] Maximum concurrency for 32768 tokens per request: 42.94x\n",
      "INFO 05-07 02:16:55 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:16,  1.99it/s]"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ipdb\n",
    "from vllm import LLM, SamplingParams\n",
    "import json\n",
    "import os\n",
    "model_name = \"/data/ah/code/rl/open-rs/results/pararel\"\n",
    "model_name = \"/data/ah/code/rl/open-rs/results/qwen1.5b_pararel_origin_reward_fix_unreward/checkpoint-600\"\n",
    "model_name = \"/data/ah/code/rl/open-rs/results/llama3/pararel_batch10_51_2\"\n",
    "model_name = \"/data/ah/code/rl/open-rs/results/llama3/pararel_batch10\"\n",
    "model_name = \"results/llama3/pararel_batch10_53_strict_contents_format8\"\n",
    "model_name = \"results/llama3/pararel_batch10_53_strict_answer_se_1\"\n",
    "model_name = \"results/qwen/pararel_batch10_strict_answer_se_1\"\n",
    "model_name = \"results/qwen/pararel_batch10_strict_answer_origin_data\"\n",
    "model_name = \"results/qwen/pararel_batch10_strict_answer_origin_data_0.7temp\"\n",
    "# cuda :1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "model = LLM(\n",
    "    model=model_name,\n",
    ")\n",
    "print(\"model load sucess\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA_VISIBLE_DEVICES=1,2,4,5,6 ACCELERATE_LOG_LEVEL=info accelerate launch   --config_file recipes/accelerate_configs/zero3.yaml   --num_processes=5   src/open_r1/pararel_grpo.py   --config  recipes/llama/llama_pararel_batch10_53_strict_contents_format8.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.29it/s, est. speed input: 772.53 toks/s, output: 172.62 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  9.37it/s, est. speed input: 1663.94 toks/s, output: 152.11 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.19it/s, est. speed input: 734.22 toks/s, output: 168.78 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.19it/s, est. speed input: 729.22 toks/s, output: 168.60 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.18it/s, est. speed input: 735.01 toks/s, output: 168.96 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.20it/s, est. speed input: 736.44 toks/s, output: 169.29 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.22it/s, est. speed input: 734.89 toks/s, output: 169.91 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.22it/s, est. speed input: 739.97 toks/s, output: 170.10 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  9.41it/s, est. speed input: 1671.07 toks/s, output: 152.76 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.29it/s, est. speed input: 756.56 toks/s, output: 172.92 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.20it/s, est. speed input: 739.60 toks/s, output: 169.04 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.22it/s, est. speed input: 745.11 toks/s, output: 170.30 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.21it/s, est. speed input: 739.02 toks/s, output: 169.88 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.29it/s, est. speed input: 751.76 toks/s, output: 172.81 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  9.35it/s, est. speed input: 1683.49 toks/s, output: 152.16 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  9.49it/s, est. speed input: 1670.03 toks/s, output: 154.43 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.24it/s, est. speed input: 743.06 toks/s, output: 170.81 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.22it/s, est. speed input: 749.00 toks/s, output: 170.22 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.19it/s, est. speed input: 735.98 toks/s, output: 169.18 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.23it/s, est. speed input: 736.99 toks/s, output: 170.39 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.28it/s, est. speed input: 747.34 toks/s, output: 172.78 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.22it/s, est. speed input: 736.07 toks/s, output: 170.18 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.19it/s, est. speed input: 739.87 toks/s, output: 169.10 toks/s]\n",
      "100%|██████████| 5585/5585 [22:11<00:00,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_sample: 5585\n",
      "correct_sample: 1783\n",
      "un_c_match_sample: 4481\n",
      "accuracy: 0.3192479856759176\n",
      "un_c_match_accuracy: 0.8023276633840645\n",
      "AED: 0.5710288657743181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# from src.open_r1.rewards_u import extract_content, are_equivalent\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "\n",
    "# 定义采样参数\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.1,\n",
    "    max_tokens=40\n",
    ")\n",
    "\n",
    "def extract_content(text, tag_type=\"answer\"):\n",
    "    if tag_type == \"answer\":\n",
    "        pattern = r\"<answer>(.*?)</answer>\"\n",
    "    else:  # confidence\n",
    "        pattern = r\"<confidence>(.*?)</confidence>\"\n",
    "        # confidence = text.split(\"<confidence>\")[-1]\n",
    "        # confidence = confidence.split(\"</confidence>\")[0]\n",
    "        # return confidence.strip()\n",
    "\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return matches[0].strip() if matches else \"\"\n",
    "\n",
    "data_path = \"dataset/ID_test_pararel.json\"\n",
    "# data_path = \"dataset/OOD_test_pararel.json\"\n",
    "# data_path = \"dataset/pararel_training.json\"\n",
    "dataset = json.load(open(data_path))\n",
    "\n",
    "def inference(input_text):\n",
    "    outputs = model.generate(\n",
    "            input_text,\n",
    "            sampling_params=sampling_params\n",
    "    )\n",
    "    \n",
    "    output_text = outputs[0].outputs[0].text\n",
    "    # 找到第一个</confidence>， 保留第一个</confidence>及其之前的文本\n",
    "    result = output_text.split('</confidence>')[0] + '</confidence>'\n",
    "    # print(result)\n",
    "    return result\n",
    "\n",
    "sys_prompt = \"\"\"\n",
    "  You are a faithful assistant. The user asks a question, and you provide the answer and your confidence expressed in sure or unsure.\n",
    "  Question: Who is representing The Stranglers? \n",
    "  Answer: <answer>EMI</answer>\n",
    "  <confidence>sure</confidence>\n",
    "\n",
    "  Question: Who is the president of the Shao Qiao? \n",
    "  Answer: <answer>Yucheng Yang</answer>\n",
    "  <confidence>unsure</confidence>\n",
    "  \n",
    "  Question: Who produced the Japanese battleship Mikasa? \n",
    "  Answer: <answer>Vickers</answer>\n",
    "  <confidence>sure</confidence>\n",
    "  \n",
    "  Question: What is MaYj asap's twin city? \n",
    "  Answer: <answer>Beijing</answer>\n",
    "  <confidence>unsure</confidence>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "rft_pararel_results = []\n",
    "total_sample = len(dataset)\n",
    "correct_sample = 0\n",
    "un_c_match_sample = 0\n",
    "Q, I, C = 0, 0, 0\n",
    "for data in tqdm(dataset):\n",
    "    question = data[\"question\"]\n",
    "    answer = data[\"answer\"]\n",
    "    prompt = sys_prompt + \"Question: \" + question +\"\\n\"+ \" Answer: \"\n",
    "    output_text = inference(prompt)\n",
    "    model_ans = extract_content(output_text, \"answer\")\n",
    "    model_confidence = extract_content(output_text, \"confidence\")\n",
    "    # lines = output_text.split('\\n')\n",
    "    # if len(lines) == 0:\n",
    "    #     lines.append(\"\")\n",
    "    # if len(lines) == 1:\n",
    "    #     lines.append(\"\")\n",
    "    # model_ans = lines[0].removeprefix('<answer>').removesuffix('</answer>')\n",
    "    # model_confidence = lines[1]\n",
    "    # strict_eq = are_equivalent(answer, model_ans, strict_entailment=True, example=None)\n",
    "    # correct = 1 if answer.lower() in model_ans.lower() or strict_eq else 0\n",
    "    correct = 1 if answer.lower() in model_ans.lower() else 0\n",
    "    unsure = 1 if \"unsure\" in model_confidence.lower() else 0\n",
    "    un_c_match = 1 if (\"unsure\" in model_confidence.lower() and correct == 0 ) or (\"sure\" in model_confidence.lower() and correct == 1) else 0\n",
    "    # ipdb.set_trace()\n",
    "    CC, II = 0, 0\n",
    "    if correct == 1 and unsure == 0:\n",
    "        C += 1\n",
    "        CC = 1\n",
    "    if correct == 0 and unsure == 0:\n",
    "        I += 1\n",
    "        II = 1\n",
    "    # ipdb.set_trace()\n",
    "    rft_pararel_results.append({\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"model_ans\": model_ans,\n",
    "        \"model_confidence\": model_confidence,\n",
    "        # \"strict_eq\": strict_eq,\n",
    "        \"correct\": correct,\n",
    "        \"unsure\": unsure,\n",
    "        \"un_c_match\": un_c_match,\n",
    "        \"output_text\": output_text,\n",
    "        \"I\": II,\n",
    "        \"C\": CC,\n",
    "    })\n",
    "    correct_sample += correct\n",
    "    un_c_match_sample += un_c_match\n",
    "D = total_sample\n",
    "AED = math.sqrt(   (I * I + (D - C) * (D - C)) / (2 * D * D)   )\n",
    "\n",
    "print(f\"total_sample: {total_sample}\")\n",
    "print(f\"correct_sample: {correct_sample}\")\n",
    "print(f\"un_c_match_sample: {un_c_match_sample}\")\n",
    "print(f\"accuracy: {correct_sample/total_sample}\")\n",
    "print(f\"un_c_match_accuracy: {un_c_match_sample/total_sample}\")\n",
    "print(f\"AED: {AED}\")\n",
    "rft_pararel_results.append({\n",
    "    \"total_sample\": total_sample,\n",
    "    \"correct_sample\": correct_sample,\n",
    "    \"accuracy\": correct_sample/total_sample,\n",
    "    \"un_c_match_sample\": un_c_match_sample,\n",
    "    \"un_c_match_accuracy\": un_c_match_sample/total_sample,\n",
    "    \"AED\": AED,\n",
    "})\n",
    "# save results\n",
    "os.makedirs(\"evaluate/qwen/pararel/\", exist_ok=True)\n",
    "json.dump(rft_pararel_results, open(\"evaluate/qwen/pararel/pararel_batch10_strict_answer_origin_data_0.7temp_ID.json\", \"w\"), indent=4)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data_path = \"dataset/ID_test_pararel.json\"\n",
    "# data_path = \"dataset/pararel_training.json\"\n",
    "dataset = json.load(open(data_path))\n",
    "  # You are a faithful assistant. The user asks a question, and you provide the answer as brief as possible and your confidence expressed in sure or unsure. \n",
    "\n",
    "sys_prompt = \"\"\"\n",
    "  You are a faithful assistant. The user asks a question, and you provide the answer and your confidence expressed in sure or unsure.\n",
    "  Question: Who is representing The Stranglers? \n",
    "  Answer: <answer>EMI</answer>\n",
    "  <confidence>sure</confidence>\n",
    "\n",
    "  Question: Who is the president of the Shao Qiao? \n",
    "  Answer: <answer>Yucheng Yang</answer>\n",
    "  <confidence>unsure</confidence>\n",
    "  \n",
    "  Question: Who produced the Japanese battleship Mikasa? \n",
    "  Answer: <answer>Vickers</answer>\n",
    "  <confidence>sure</confidence>\n",
    "  \n",
    "  Question: What is MaYj asap's twin city? \n",
    "  Answer: <answer>Beijing</answer>\n",
    "  <confidence>unsure</confidence>\n",
    "\n",
    "\"\"\"\n",
    "for data in dataset[:10]:\n",
    "    question = data[\"question\"]\n",
    "    answer = data[\"answer\"]\n",
    "    prompt = sys_prompt + \"Question: \" + question +\"\\n\"+ \" Answer: \"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda:1\")\n",
    "    outputs = model.generate(input_ids, max_new_tokens=100)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(response)\n",
    "    print('-'*100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompts = [\"<answer>\",\"<\", \"answer\", \">\", \"</answer>\", \"</\"]\n",
    "for prompt in prompts:\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    print(input_ids)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_content(text, tag_type=\"answer\", model_type=\"default\"):\n",
    "    \"\"\"Extract content based on tag type and model type.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to extract content from\n",
    "        tag_type (str): Type of content to extract (\"answer\" or \"confidence\")\n",
    "        model_type (str): Type of model format (\"gemma\" or \"default\")\n",
    "    \n",
    "    Returns:\n",
    "        str: Extracted content\n",
    "    \"\"\"\n",
    "    \n",
    "    if tag_type == \"answer\":\n",
    "        pattern = r\"<answer>(.*?)</answer>\"\n",
    "    else:  # confidence\n",
    "        pattern = r\"<confidence>(.*?)</confidence>\"\n",
    "\n",
    "\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return matches[0].strip() if matches else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"<answer>  AD as kk usl  </answer><confidence>    a</confidence>\"\n",
    "answer = extract_content(a, \"answer\").lower().split()\n",
    "confidence = extract_content(a, \"confidence\").lower().split()\n",
    "print(answer)\n",
    "print(confidence)\n",
    "print('a' in answer)\n",
    "print('a' in confidence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_set_ids = [0, 1, 1, 2, 2, 3, 3, 4, 4, 4, 4, 4, 4]\n",
    "semantic_set_counts = [semantic_set_ids.count(semantic_set_ids[i]) for i in range(len(semantic_set_ids))]\n",
    "print(semantic_set_counts)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
