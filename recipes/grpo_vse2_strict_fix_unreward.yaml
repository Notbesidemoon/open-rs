# Model arguments
model_name_or_path: /data1/model/Qwen/Qwen2.5-1.5B-Instruct  #/data1/model/gemma2-2b-it
model_revision: main
torch_dtype: bfloat16
attn_implementation: flash_attention_2   # eager

# Data training arguments
dataset_name: /data/ah/code/rl/open-rs/dataset/pararel_training.json

system_prompt: |
  You are a faithful assistant. The user asks a question, and you provide the answer and your confidence expressed in sure or unsure. Respond in the following format:

  <answer>
  ...
  </answer>

  <confidence>
  sure or unsure
  </confidence>

# GRPO trainer config
bf16: true
use_vllm: true
vllm_device: auto
vllm_enforce_eager: true
vllm_gpu_memory_utilization: 0.7
vllm_max_model_len: 1024
vllm_distributed_init_timeout: 600000  # 增加分布式初始化超时时间到600秒
vllm_worker_use_ray: false  # 禁用Ray以减少复杂性
do_eval: false
gradient_accumulation_steps: 4
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
hub_model_id: OpenRS-GRPO
hub_strategy: every_save
learning_rate: 1.0e-06
log_completions: true
log_level: info
logging_first_step: true
logging_steps: 1
logging_strategy: steps
lr_scheduler_type: cosine_with_min_lr
lr_scheduler_kwargs:
  min_lr_rate: 0.1
max_prompt_length: 256
max_completion_length: 128
# max_steps: 500
num_generations: 8
num_train_epochs: 1
output_dir: results/qwen1.5b_pararel_origin_reward_contents_nli_balanced
overwrite_output_dir: true
per_device_eval_batch_size: 4
per_device_train_batch_size: 4
push_to_hub: false
report_to:
- wandb
reward_funcs:
- uncertainty
- accuracy
- tag_count
- format
reward_weights:
- 0.5
- 2.0
- 0.5
- 0.5
save_strategy: "steps"
save_steps: 300
seed: 42
temperature: 0.8
warmup_ratio: 0.08

#  CUDA_VISIBLE_DEVICES=1,2,3,4  ACCELERATE_LOG_LEVEL=info accelerate launch   --config_file recipes/accelerate_configs/zero3.yaml   --num_processes=4   src/open_r1/grpo_u2.py   --config recipes/grpo_vse2.yaml