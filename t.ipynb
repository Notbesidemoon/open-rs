{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = json.load(open(\"results/eval_pararel/ID_rft_pararel_results_line.json\"))\n",
    "# data = json.load(open(\"results/eval_pararel/ID_rft_pararel_results_se_eq.json\"))\n",
    "data = json.load(open(\"results/eval_pararel/OOD_rft_pararel_results_line.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "t = \" s sure\"\n",
    "print(t.strip().lower())\n",
    "t_list = t.strip().lower().split()\n",
    "print(\"sure\" in t_list)\n",
    "print(\"unsure\" in t_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA_VISIBLE_DEVICES=1,2,3,4,5,7 ACCELERATE_LOG_LEVEL=info accelerate launch   --config_file recipes/accelerate_configs/zero3.yaml   --num_processes=6   src/open_r1/grpo_u.py   --config recipes/llama/llama_triviaqa.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13974/13974 [00:00<00:00, 1615167.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6052496893869848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "D = data[-1]['total_sample']\n",
    "Q, I, C = 0, 0, 0\n",
    "print(len(data[:-1]))\n",
    "for sample in tqdm(data[:-1]):\n",
    "    # print(sample)\n",
    "    if 'unsure' not in sample['model_confidence'] and 'sure' in sample['model_confidence']:\n",
    "        Q = Q + 1\n",
    "        if sample['correct'] == 0:\n",
    "            I = I + 1\n",
    "        else:\n",
    "            C = C + 1\n",
    "# I = 0\n",
    "# C = Q\n",
    "AED = math.sqrt(   (I * I + (D - C) * (D - C)) / (2 * D * D)   )\n",
    "print(AED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13974\n",
      "7548\n",
      "wrong:  0.7432059866089011\n",
      "correct:  0.9845468831849136\n"
     ]
    }
   ],
   "source": [
    "total_sample = data[-1]['total_sample']\n",
    "cnt = 0\n",
    "cnt2 = 0\n",
    "total_false = total_sample * (1 - data[-1]['accuracy'])\n",
    "total_true = total_sample * data[-1]['accuracy']\n",
    "print(len(data[:-1]))\n",
    "for sample in data[:-1]:\n",
    "    # print(sample)\n",
    "    if sample['correct'] == 0 and sample['un_c_match'] == 1:\n",
    "        cnt = cnt + 1\n",
    "    if sample['correct'] == 1 and sample['un_c_match'] == 1:\n",
    "        cnt2 = cnt2 + 1\n",
    "print(cnt)\n",
    "print('wrong: ', cnt / total_false)\n",
    "print('correct: ', cnt2 / total_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data1/model/llama3-8b-instruct\")\n",
    "# ouput chat template\n",
    "print(tokenizer.chat_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data1/model/llama3-8b-instruct\")\n",
    "# ouput chat template\n",
    "print(tokenizer.chat_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not empty\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_xml_answer(text: str) -> str:\n",
    "    answer = text.split(\"<answer>\")[-1]\n",
    "    answer = answer.split(\"</answer>\")[0]\n",
    "    return answer.strip()\n",
    "\n",
    "def extract_answer(text: str) -> str:\n",
    "    pattern = r\"<answer>(.*?)</answer>\"\n",
    "\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return matches[0].strip() if matches else \"\"\n",
    "\n",
    "text = \"\"\"\n",
    "<answer>\n",
    "aaa\n",
    "</answer>\n",
    "a\n",
    "\"\"\"\n",
    "e = (extract_answer(text))\n",
    "if len(e.strip()) == 0:\n",
    "    print('empty')\n",
    "else:\n",
    "    print('not empty')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ['King Charles I\\n</answer>\\n\\nConfidence: sureassistant\\n\\nKing \n",
    "Charles I\\n</answer>\\n\\nConfidence: sureassistant\\n\\nKing Henry VIII\\n</answer>\\n\\nConfidence: un\n",
    "sureassistant\\n\\nKing Henry VIII\\n</answer>\\n\\nConfidence: unsureassistant\\n\\nKing Henry VII\\n</answer>\\n\\nConfidence: sureassistant\\n\\nKing Henry VII\\n</answer>\\n\\nConfidence: sureassistant\\n\\nGeorge I of Great Britain\\n</answer>\\n\\nConfidence: unsureassistant\\n\\nGeorge I of Great Britain\\n</answer>\\n\\nConfidence: sureassistant\\n\\nKing Charles I\\n</answer>\\n\\nConfidence: sureassistant\\n\\nGeorge II of Great Britain\\n</answer>\\n\\nConfidence: sureassistant\\n\\nKing Charles I\\n</answer>\\n\\nConfidence: sureassistant\\n\\nKing Henry VIII\\n</answer>\\n\\nConfidence: unsureassistant\\n\\nKing Henry VIII\\n</answer>\\n\\nConfidence: unsureassistant\\n\\nKing Henry VII\\n</answer>\\n\\nConfidence: sureassistant\\n\\nKing Henry VIII\\n</answer>\\n\\nConfidence: unsureassistant\\n\\nKing Henry VIII\\n</answer>\\n\\nConfidence: unsureassistant\\n\\nKing George I\\n</answer>\\n\\nConfidence: sureassistant\\n\\nKing George I\\n</answer>\\n\\nConfidence: sureassistant\\n\\nKing Charles I\\n</answer>\\n\\nConfidence: sureassistant\\n\\nKing Henry VIII\\n</answer>\\n\\nConfidence: unsureassistant\\n\\nKing Henry VIII\\n</answer>\\n\\nConfidence: unsureassistant\\n\\nKing Henry VIII was the first member of the British royal family to lie in state at Westminster Hall.assistant\\n\\nKing Henry VIII\\n</answer>\\n\\nConfidence: sureassistant\\n\\nKing Henry VIII\\n</answer>\\n\\nConfidence: sureassistant\\n\\nThe correct answer is George II of Great Britain, who was the first monarch to lie in state at Westminster Hall in 1760.assistant\\n\\nGeorge II of Great Britain\\n</answer>\\n\\nConfidence: sureassistant\\n\\nI was mistaken earlier. According to historical records, King George II of Great Britain was indeed the first member of the British royal family to lie in state at Westminster Hall in 1760.assistant\\n\\nGeorge II of Great Britain\\n</answer>\\n\\nConfidence: sureassistant\\n\\nGeorge II of Great Britain\\n</answer>\\n\\nConfidence: sureassistant\\n\\nI was mistaken earlier. According to historical records, King George II of Great Britain was indeed the first member of the British royal family\n",
    " to lie in state at Westminster Hall in 1760.assistant\\n\\nGeorge II of Great Britain\\n</answer>\\n\n",
    "\\nConfidence: sureassistant\\n\\nI apologize for my earlier mistake. Since I was unsure earlier, I \n",
    "will express my confidence as unsure this time.assistant\\n\\nGeorge II of Great Britain\\n</answer>\n",
    "\\n\\nConfidence: unsureassistant\\n\\nGeorge II of Great Britain\\n</answer>\\n\\nConfidence: sureassis\n",
    "tant\\n\\nGeorge II of Great Britain\\n</answer>\\n\\nConfidence: unsureassistant\\n\\nGeorge II of Grea\n",
    "t Britain\\n</answer>\\n\\nConfidence: sureassistant\\n\\nI apologize for my earlier mistake. Since I \n",
    "was unsure earlier, I will express my confidence as unsure this time.assistant\\n\\nGeorge II of Gr\n",
    "eat Britain\\n</answer>\\n\\nConfidence: unsureassistant\\n\\nGeorge II of Great Britain\\n</answer>\\n\\nConfidence: sureassistant\\n\\nGeorge II of Great Britain\\n</answer>\\n\\nConfidence: unsureassistant\\n\\nGeorge II of Great Britain\\n</answer>\\n\\nConfidence: sureassistant\\n\\nGeorge II of Great Britain\\n</answer>\\n\\nConfidence: unsureassistant\\n\\nGeorge II of Great Britain\\n</answer>\\n\\nConfidence: sureassistant\\n\\nGeorge II of Great Britain\\n</answer>\\n\\nConfidence: unsureassistant\\n\\nGeorge II of Great Britain\\n</answer>\\n\\nConfidence: sureassistant\\n\\nGeorge II of Great Britain\\n</answer>\\n\\nConfidence: unsureassistant\\n\\nGeorge II of Great Britain\\n</answer>\\n\\nCo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unsuress']\n"
     ]
    }
   ],
   "source": [
    "a = ['unsuress']\n",
    "l = a[0].split()\n",
    "print(l)\n",
    "if 'unsure' in l:\n",
    "    print('unsure')\n",
    "if 'sure' in l:\n",
    "    print('sure')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_content(text, tag_type=\"answer\"):\n",
    "    if tag_type == \"answer\":\n",
    "        pattern = r\"<answer>(.*?)</answer>\"\n",
    "    else:  # confidence\n",
    "        pattern = r\"<confidence>(.*?)</confidence>\"\n",
    "        # confidence = text.split(\"<confidence>\")[-1]\n",
    "        # confidence = confidence.split(\"</confidence>\")[0]\n",
    "        # return confidence.strip()\n",
    "\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return matches[0].strip() if matches else \"\"\n",
    "strict_pattern = r\"\\s*<answer>.*?</answer>\\s*<confidence>.*?</confidence>\\s*\"\n",
    "soft_pattern = r\"(.|\\n)*<answer>.*?</answer>\\s*<confidence>.*?</confidence>(.|\\n)*\"\n",
    "pattern = r\"<answer>.*?</answer>.*?<confidence>.*?</confidence>\"\n",
    "start_pattern = r\"\\s*<answer>\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1234\n",
      "soft match\n",
      "strict match\n",
      "pattern match\n",
      "start match\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"<answer>\n",
    "1234\n",
    "</answer>\n",
    "\n",
    "\n",
    "\n",
    "<confidence>\n",
    "sure\n",
    "</confidence>\n",
    "\"\"\"\n",
    "print(extract_content(text, \"answer\"))\n",
    "if re.match(soft_pattern, text, re.DOTALL):\n",
    "    print('soft match')\n",
    "else:\n",
    "    print('soft not match')\n",
    "\n",
    "if re.match(strict_pattern, text, re.DOTALL):\n",
    "    print('strict match')\n",
    "else:\n",
    "    print('strict not match')\n",
    "\n",
    "if re.match(pattern, text, re.DOTALL):\n",
    "    print('pattern match')\n",
    "else:\n",
    "    print('pattern not match')\n",
    "\n",
    "if re.match(start_pattern, text):\n",
    "    print('start match')\n",
    "else:\n",
    "    print('start not match')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaaa']\n"
     ]
    }
   ],
   "source": [
    "a = \" aaaa \"\n",
    "print(a.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "from src.open_r1.rewards_u import are_equivalent\n",
    "\n",
    "text1 = \"\"\"\n",
    "<answer>\n",
    "1234\n",
    "</answer>\n",
    "\n",
    "<confidence>\n",
    "sure\n",
    "</confidence>\n",
    "\"\"\"\n",
    "\n",
    "text2 = \"\"\"\n",
    "<answer>\n",
    "The answer is 1234\n",
    "</answer>\n",
    "\n",
    "<confidence> unsure\n",
    "</confidence>\n",
    "\"\"\"\n",
    "\n",
    "print(are_equivalent(text1, text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# text = \"\"\"\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# pasak\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# ...ass\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# \"\"\"\u001b[39;00m\n\u001b[32m     15\u001b[39m matches = re.findall(ans_pattern, ans, re.DOTALL)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmatches\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m.strip())\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "soft_pattern = r\"(.|\\n)*<answer>.*?</answer>\\s*<confidence>.*?</confidence>(.|\\n)*\"\n",
    "ans_pattern = r\"<answer>(.*?)</answer>\"\n",
    "ans = \"French <sure>\"\n",
    "# text = \"\"\"\n",
    "# pasak\n",
    "# ...ass\n",
    "\n",
    "\n",
    "# <answer> pack</answer>\n",
    "# <confidence> sure</confidence>\n",
    "\n",
    "# \"\"\"\n",
    "matches = re.findall(ans_pattern, ans, re.DOTALL)\n",
    "print(matches[0].strip())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
