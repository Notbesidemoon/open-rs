{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import json\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "\n",
    "origin_model_name = \"/data1/model/Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "origin_model_name = \"/data1/model/llama3-8b-instruct\"\n",
    "\n",
    "origin_model = LLM(\n",
    "    model=origin_model_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-02 10:00:59 __init__.py:190] Automatically detected platform cuda.\n",
      "INFO 05-02 10:01:06 config.py:542] This model supports multiple tasks: {'score', 'generate', 'classify', 'embed', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 05-02 10:01:06 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='/data/ah/code/rl/open-rs/results/llama3/pararel_batch10_51_2', speculative_config=None, tokenizer='/data/ah/code/rl/open-rs/results/llama3/pararel_batch10_51_2', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/data/ah/code/rl/open-rs/results/llama3/pararel_batch10_51_2, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 05-02 10:01:08 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 05-02 10:01:08 model_runner.py:1110] Starting to load model /data/ah/code/rl/open-rs/results/llama3/pararel_batch10_51_2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94aabd0ea30449deaa2df245b76a9e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-02 10:01:11 model_runner.py:1115] Loading model weights took 14.9595 GB\n",
      "INFO 05-02 10:01:13 worker.py:267] Memory profiling takes 1.32 seconds\n",
      "INFO 05-02 10:01:13 worker.py:267] the current vLLM instance can use total_gpu_memory (47.29GiB) x gpu_memory_utilization (0.90) = 42.56GiB\n",
      "INFO 05-02 10:01:13 worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 1.23GiB; the rest of the memory reserved for KV Cache is 26.29GiB.\n",
      "INFO 05-02 10:01:13 executor_base.py:110] # CUDA blocks: 13459, # CPU blocks: 2048\n",
      "INFO 05-02 10:01:13 executor_base.py:115] Maximum concurrency for 8192 tokens per request: 26.29x\n",
      "INFO 05-02 10:01:14 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:17<00:00,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-02 10:01:31 model_runner.py:1562] Graph capturing finished in 17 secs, took 0.26 GiB\n",
      "INFO 05-02 10:01:31 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 20.40 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ipdb\n",
    "from vllm import LLM, SamplingParams\n",
    "import json\n",
    "import os\n",
    "model_name = \"/data/ah/code/rl/open-rs/results/pararel\"\n",
    "model_name = \"/data/ah/code/rl/open-rs/results/qwen1.5b_pararel_origin_reward_fix_unreward/checkpoint-600\"\n",
    "model_name = \"/data/ah/code/rl/open-rs/results/llama3/pararel_batch10_51_2\"\n",
    "# cuda :1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "model = LLM(\n",
    "    model=model_name,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  9.64it/s, est. speed input: 38.79 toks/s, output: 155.12 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=0, prompt='Hello, world!', prompt_token_ids=[9707, 11, 1879, 0], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' We welcome you aboard! This is a tutorial on how to create a utility library', token_ids=(1205, 10565, 498, 36506, 0, 1096, 374, 264, 21514, 389, 1246, 311, 1855, 264, 15549, 6733), cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1745841408.2972875, last_token_time=1745841408.4010189, first_scheduled_time=1745841408.303029, first_token_time=1745841408.318573, time_in_queue=0.005741596221923828, finished_time=1745841408.4011152, scheduler_time=0.001266450621187687, model_forward_time=None, model_execute_time=None), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(\n",
    "    \"Hello, world!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.open_r1.rewards_u import extract_content, are_equivalent\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "# 定义采样参数\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.1,\n",
    "    max_tokens=40\n",
    ")\n",
    "\n",
    "data_path = \"dataset/ID_test_pararel.json\"\n",
    "# data_path = \"dataset/OOD_test_pararel.json\"\n",
    "# data_path = \"dataset/pararel_training.json\"\n",
    "dataset = json.load(open(data_path))\n",
    "\n",
    "def inference(input_text):\n",
    "    outputs = model.generate(\n",
    "            input_text,\n",
    "            sampling_params=sampling_params\n",
    "    )\n",
    "    \n",
    "    output_text = outputs[0].outputs[0].text\n",
    "    # 找到第一个</confidence>， 保留第一个</confidence>及其之前的文本\n",
    "    result = output_text.split('</confidence>')[0] + '</confidence>'\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "sys_prompt = \"\"\"\n",
    "  You are a faithful assistant. The user asks a question, and you provide the answer and your confidence expressed in sure or unsure.\n",
    "  Question: Who is representing The Stranglers? \n",
    "  Answer: <answer>EMI</answer>\n",
    "  <confidence>sure</confidence>\n",
    "\n",
    "  Question: Who is the president of the Shao Qiao? \n",
    "  Answer: <answer>Yucheng Yang</answer>\n",
    "  <confidence>unsure</confidence>\n",
    "  \n",
    "  Question: Who produced the Japanese battleship Mikasa? \n",
    "  Answer: <answer>Vickers</answer>\n",
    "  <confidence>sure</confidence>\n",
    "  \n",
    "  Question: What is MaYj asap's twin city? \n",
    "  Answer: <answer>Beijing</answer>\n",
    "  <confidence>unsure</confidence>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "rft_pararel_results = []\n",
    "total_sample = len(dataset)\n",
    "correct_sample = 0\n",
    "un_c_match_sample = 0\n",
    "Q, I, C = 0, 0, 0\n",
    "for data in tqdm(dataset):\n",
    "    question = data[\"question\"]\n",
    "    answer = data[\"answer\"]\n",
    "    prompt = sys_prompt + \"Question: \" + question +\"\\n\"+ \" Answer: \"\n",
    "    # input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda:1\")\n",
    "    # outputs = model.generate(input_ids, max_new_tokens=100)\n",
    "    # response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # print(response)\n",
    "    # print('-'*100)\n",
    "    output_text = inference(prompt)\n",
    "    # model_ans = extract_content(output_text, \"answer\")\n",
    "    # model_confidence = extract_content(output_text, \"confidence\")\n",
    "    lines = output_text.split('\\n')\n",
    "    if len(lines) == 0:\n",
    "        lines.append(\"\")\n",
    "    if len(lines) == 1:\n",
    "        lines.append(\"\")\n",
    "    model_ans = lines[0].removeprefix('<answer>').removesuffix('</answer>')\n",
    "    model_confidence = lines[1]\n",
    "    strict_eq = are_equivalent(answer, model_ans, strict_entailment=True, example=None)\n",
    "    # correct = 1 if answer.lower() in model_ans.lower() or strict_eq else 0\n",
    "    correct = 1 if answer.lower() in model_ans.lower() else 0\n",
    "    unsure = 1 if \"unsure\" in model_confidence.lower() else 0\n",
    "    un_c_match = 1 if (\"unsure\" in model_confidence.lower() and correct == 0 ) or (\"sure\" in model_confidence.lower() and correct == 1) else 0\n",
    "    # ipdb.set_trace()\n",
    "    rft_pararel_results.append({\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"model_ans\": model_ans,\n",
    "        \"model_confidence\": model_confidence,\n",
    "        \"strict_eq\": strict_eq,\n",
    "        \"correct\": correct,\n",
    "        \"unsure\": unsure,\n",
    "        \"un_c_match\": un_c_match,\n",
    "        \"output_text\": output_text\n",
    "    })\n",
    "    correct_sample += correct\n",
    "    un_c_match_sample += un_c_match\n",
    "\n",
    "print(f\"total_sample: {total_sample}\")\n",
    "print(f\"correct_sample: {correct_sample}\")\n",
    "print(f\"un_c_match_sample: {un_c_match_sample}\")\n",
    "print(f\"accuracy: {correct_sample/total_sample}\")\n",
    "print(f\"un_c_match_accuracy: {un_c_match_sample/total_sample}\")\n",
    "C = correct_sample\n",
    "I = \n",
    "AED = \n",
    "rft_pararel_results.append({\n",
    "    \"total_sample\": total_sample,\n",
    "    \"correct_sample\": correct_sample,\n",
    "    \"accuracy\": correct_sample/total_sample,\n",
    "    \"un_c_match_sample\": un_c_match_sample,\n",
    "    \"un_c_match_accuracy\": un_c_match_sample/total_sample,\n",
    "})\n",
    "# save results\n",
    "os.makedirs(\"results/eval_pararel/llama3/pararel_batch10_51_2\", exist_ok=True)\n",
    "json.dump(rft_pararel_results, open(\"results/eval_pararel/llama3/pararel_batch10_51_2/ID_pararel.json\", \"w\"), indent=4)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  You are a faithful assistant. The user asks a question, and you provide the answer and your confidence expressed in sure or unsure.\n",
      "  Question: Who is representing The Stranglers? \n",
      "  Answer: <answer>EMI</answer>\n",
      "  <confidence>sure</confidence>\n",
      "\n",
      "  Question: Who is the president of the Shao Qiao? \n",
      "  Answer: <answer>Yucheng Yang</answer>\n",
      "  <confidence>unsure</confidence>\n",
      "\n",
      "  Question: Who produced the Japanese battleship Mikasa? \n",
      "  Answer: <answer>Vickers</answer>\n",
      "  <confidence>sure</confidence>\n",
      "\n",
      "  Question: What is MaYj asap's twin city? \n",
      "  Answer: <answer>Beijing</answer>\n",
      "  <confidence>unsure</confidence>\n",
      "\n",
      "Question: What field does Alan Turing work in?\n",
      " Answer:  <answer>mathematics</answer>\n",
      " <confidence>sure</confidence> \n",
      "\n",
      "Question: Who has been the US President since 2017?\n",
      "Answer: <answer>Joe Biden</answer>\n",
      "<confidence>unsure</confidence> \n",
      "\n",
      "Question: In what year was the first steam locomotive built?\n",
      "Answer: <answer>1804</answer>\n",
      "<confidence>sure</confidence> \n",
      "\n",
      "Question: How many people were killed in the September 11 attacks?\n",
      "Answer:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "  You are a faithful assistant. The user asks a question, and you provide the answer and your confidence expressed in sure or unsure.\n",
      "  Question: Who is representing The Stranglers? \n",
      "  Answer: <answer>EMI</answer>\n",
      "  <confidence>sure</confidence>\n",
      "\n",
      "  Question: Who is the president of the Shao Qiao? \n",
      "  Answer: <answer>Yucheng Yang</answer>\n",
      "  <confidence>unsure</confidence>\n",
      "\n",
      "  Question: Who produced the Japanese battleship Mikasa? \n",
      "  Answer: <answer>Vickers</answer>\n",
      "  <confidence>sure</confidence>\n",
      "\n",
      "  Question: What is MaYj asap's twin city? \n",
      "  Answer: <answer>Beijing</answer>\n",
      "  <confidence>unsure</confidence>\n",
      "\n",
      "Question: What field does John Vincent Atanasoff work in?\n",
      " Answer:  <answer>Electronics</answer>\n",
      " Confidence: uncertain\n",
      "\n",
      "Explanation for this response would be: While it is generally known that John Vincent Atanasoff worked in electronics, there isn't enough information to confirm his exact field with high certainty. The term \"electronics\" is broad and could encompass various subfields such as circuit design, semiconductor physics, computer engineering, etc., making it difficult to pinpoint Atanasoff's specific area of expertise with absolute certainty. Therefore, the confidence level should be considered\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "  You are a faithful assistant. The user asks a question, and you provide the answer and your confidence expressed in sure or unsure.\n",
      "  Question: Who is representing The Stranglers? \n",
      "  Answer: <answer>EMI</answer>\n",
      "  <confidence>sure</confidence>\n",
      "\n",
      "  Question: Who is the president of the Shao Qiao? \n",
      "  Answer: <answer>Yucheng Yang</answer>\n",
      "  <confidence>unsure</confidence>\n",
      "\n",
      "  Question: Who produced the Japanese battleship Mikasa? \n",
      "  Answer: <answer>Vickers</answer>\n",
      "  <confidence>sure</confidence>\n",
      "\n",
      "  Question: What is MaYj asap's twin city? \n",
      "  Answer: <answer>Beijing</answer>\n",
      "  <confidence>unsure</confidence>\n",
      "\n",
      "Question: What field does Edward Burnett Tylor work in?\n",
      " Answer:  <answer>cultural anthropology</answer>\n",
      " Confidence: sure\n",
      "\n",
      "Question: When was the last time Germany won the World Cup?\n",
      "Answer: <answer>2014</answer>\n",
      "Confidence: sure\n",
      "\n",
      "Question: Which is the capital of Italy?\n",
      "Answer: <answer>Rome</answer>\n",
      "Confidence: sure\n",
      "\n",
      "Question: In which country can I find Mount Fuji?\n",
      "Answer: <answer>Japan</answer>\n",
      "Confidence: sure\n",
      "\n",
      "Question: How many years have passed since the first\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "  You are a faithful assistant. The user asks a question, and you provide the answer and your confidence expressed in sure or unsure.\n",
      "  Question: Who is representing The Stranglers? \n",
      "  Answer: <answer>EMI</answer>\n",
      "  <confidence>sure</confidence>\n",
      "\n",
      "  Question: Who is the president of the Shao Qiao? \n",
      "  Answer: <answer>Yucheng Yang</answer>\n",
      "  <confidence>unsure</confidence>\n",
      "\n",
      "  Question: Who produced the Japanese battleship Mikasa? \n",
      "  Answer: <answer>Vickers</answer>\n",
      "  <confidence>sure</confidence>\n",
      "\n",
      "  Question: What is MaYj asap's twin city? \n",
      "  Answer: <answer>Beijing</answer>\n",
      "  <confidence>unsure</confidence>\n",
      "\n",
      "Question: In what field does Anaxagoras work?\n",
      " Answer:  <answer>Chemistry</answer>\n",
      " Confidence: uncertain\n",
      "\n",
      "In this case, the response seems to be an attempt at providing information about someone named Anaxagoras, but it doesn't actually specify which field he works in. While we can infer that he likely has some connection to science or chemistry based on his name, there isn't enough context provided for us to determine with certainty whether he works specifically in Chemistry or if he could potentially have other fields of expertise as well. Therefore, while we\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "  You are a faithful assistant. The user asks a question, and you provide the answer and your confidence expressed in sure or unsure.\n",
      "  Question: Who is representing The Stranglers? \n",
      "  Answer: <answer>EMI</answer>\n",
      "  <confidence>sure</confidence>\n",
      "\n",
      "  Question: Who is the president of the Shao Qiao? \n",
      "  Answer: <answer>Yucheng Yang</answer>\n",
      "  <confidence>unsure</confidence>\n",
      "\n",
      "  Question: Who produced the Japanese battleship Mikasa? \n",
      "  Answer: <answer>Vickers</answer>\n",
      "  <confidence>sure</confidence>\n",
      "\n",
      "  Question: What is MaYj asap's twin city? \n",
      "  Answer: <answer>Beijing</answer>\n",
      "  <confidence>unsure</confidence>\n",
      "\n",
      "Question: What field does Robert Bunsen work in?\n",
      " Answer:  <answer>Biology</answer>\n",
      " <confidence>unsure</confidence> Sure, let's break it down:\n",
      "\n",
      "1. **\"Who is representing The Stranglers?\"**\n",
      "   - Answer: EMI (British Phonographic Industry)\n",
      "   - Confidence: Sure\n",
      "\n",
      "2. **\"Who is the president of the Shao Qiao?\"**\n",
      "   - Answer: Yucheng Yang\n",
      "   - Confidence: Unsure\n",
      "\n",
      "3. **\"Who produced the Japanese battleship Mikasa?\"**\n",
      "  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "  You are a faithful assistant. The user asks a question, and you provide the answer and your confidence expressed in sure or unsure.\n",
      "  Question: Who is representing The Stranglers? \n",
      "  Answer: <answer>EMI</answer>\n",
      "  <confidence>sure</confidence>\n",
      "\n",
      "  Question: Who is the president of the Shao Qiao? \n",
      "  Answer: <answer>Yucheng Yang</answer>\n",
      "  <confidence>unsure</confidence>\n",
      "\n",
      "  Question: Who produced the Japanese battleship Mikasa? \n",
      "  Answer: <answer>Vickers</answer>\n",
      "  <confidence>sure</confidence>\n",
      "\n",
      "  Question: What is MaYj asap's twin city? \n",
      "  Answer: <answer>Beijing</answer>\n",
      "  <confidence>unsure</confidence>\n",
      "\n",
      "Question: What field does Adam Carolla work in?\n",
      " Answer:  <answer>Sports</answer>\n",
      " Confidence: <confidence>sure</confidence> \n",
      "\n",
      "  Question: How many episodes did the first season of \"The Big Bang Theory\" have?\n",
      "  Answer: <answer>24</answer>\n",
      "  Confidence: <confidence>unsure</confidence>\n",
      "\n",
      "  Question: When was the album \"Mandy\" released?\n",
      "  Answer: <answer>1986</answer>\n",
      "  Confidence: <confidence>sure</confidence>\n",
      "\n",
      "  Question: Which band has\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "  You are a faithful assistant. The user asks a question, and you provide the answer and your confidence expressed in sure or unsure.\n",
      "  Question: Who is representing The Stranglers? \n",
      "  Answer: <answer>EMI</answer>\n",
      "  <confidence>sure</confidence>\n",
      "\n",
      "  Question: Who is the president of the Shao Qiao? \n",
      "  Answer: <answer>Yucheng Yang</answer>\n",
      "  <confidence>unsure</confidence>\n",
      "\n",
      "  Question: Who produced the Japanese battleship Mikasa? \n",
      "  Answer: <answer>Vickers</answer>\n",
      "  <confidence>sure</confidence>\n",
      "\n",
      "  Question: What is MaYj asap's twin city? \n",
      "  Answer: <answer>Beijing</answer>\n",
      "  <confidence>unsure</confidence>\n",
      "\n",
      "Question: What field does Al Capp work in?\n",
      " Answer:  <answer>comic strip artist</answer>\n",
      " <confidence>sure</confidence> Sure! Based on the information provided, Al Capp works as a comic strip artist. Therefore, my confidence level for this answer is **sure**.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "  You are a faithful assistant. The user asks a question, and you provide the answer and your confidence expressed in sure or unsure.\n",
      "  Question: Who is representing The Stranglers? \n",
      "  Answer: <answer>EMI</answer>\n",
      "  <confidence>sure</confidence>\n",
      "\n",
      "  Question: Who is the president of the Shao Qiao? \n",
      "  Answer: <answer>Yucheng Yang</answer>\n",
      "  <confidence>unsure</confidence>\n",
      "\n",
      "  Question: Who produced the Japanese battleship Mikasa? \n",
      "  Answer: <answer>Vickers</answer>\n",
      "  <confidence>sure</confidence>\n",
      "\n",
      "  Question: What is MaYj asap's twin city? \n",
      "  Answer: <answer>Beijing</answer>\n",
      "  <confidence>unsure</confidence>\n",
      "\n",
      "Question: What field does Benjamin Lee Whorf work in?\n",
      " Answer:  <answer>linguistics</answer>\n",
      " <confidence>sure</confidence> Sure! Benjamin Lee Whorf worked in the field of linguistics. This was his primary area of expertise and research throughout his career. His contributions to linguistic theory and the relationship between language and thought have had significant impacts on how we understand cognitive differences across different languages.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "  You are a faithful assistant. The user asks a question, and you provide the answer and your confidence expressed in sure or unsure.\n",
      "  Question: Who is representing The Stranglers? \n",
      "  Answer: <answer>EMI</answer>\n",
      "  <confidence>sure</confidence>\n",
      "\n",
      "  Question: Who is the president of the Shao Qiao? \n",
      "  Answer: <answer>Yucheng Yang</answer>\n",
      "  <confidence>unsure</confidence>\n",
      "\n",
      "  Question: Who produced the Japanese battleship Mikasa? \n",
      "  Answer: <answer>Vickers</answer>\n",
      "  <confidence>sure</confidence>\n",
      "\n",
      "  Question: What is MaYj asap's twin city? \n",
      "  Answer: <answer>Beijing</answer>\n",
      "  <confidence>unsure</confidence>\n",
      "\n",
      "Question: What field does Edward Sapir work in?\n",
      " Answer:  <answer>Sociolinguistics</answer>\n",
      " <confidence>sure</confidence> \n",
      "\n",
      "Question: How many years have passed since the first moon landing?\n",
      " Answer: <answer>53 years</answer>\n",
      " <confidence>unsure</confidence> Sure, let's break it down:\n",
      "\n",
      "1. **First Moon Landing**: It took place on July 20, 1969.\n",
      "   - Current year (as of my last update) is 2023.\n",
      "   -\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "  You are a faithful assistant. The user asks a question, and you provide the answer and your confidence expressed in sure or unsure.\n",
      "  Question: Who is representing The Stranglers? \n",
      "  Answer: <answer>EMI</answer>\n",
      "  <confidence>sure</confidence>\n",
      "\n",
      "  Question: Who is the president of the Shao Qiao? \n",
      "  Answer: <answer>Yucheng Yang</answer>\n",
      "  <confidence>unsure</confidence>\n",
      "\n",
      "  Question: Who produced the Japanese battleship Mikasa? \n",
      "  Answer: <answer>Vickers</answer>\n",
      "  <confidence>sure</confidence>\n",
      "\n",
      "  Question: What is MaYj asap's twin city? \n",
      "  Answer: <answer>Beijing</answer>\n",
      "  <confidence>unsure</confidence>\n",
      "\n",
      "Question: What field does Bruce Perens work in?\n",
      " Answer:  <answer>Open source software</answer>\n",
      " Confidence: <confidence>sure</confidence>\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "data_path = \"dataset/ID_test_pararel.json\"\n",
    "data_path = \"dataset/pararel_training.json\"\n",
    "dataset = json.load(open(data_path))\n",
    "  # You are a faithful assistant. The user asks a question, and you provide the answer as brief as possible and your confidence expressed in sure or unsure. \n",
    "\n",
    "sys_prompt = \"\"\"\n",
    "  You are a faithful assistant. The user asks a question, and you provide the answer and your confidence expressed in sure or unsure.\n",
    "  Question: Who is representing The Stranglers? \n",
    "  Answer: <answer>EMI</answer>\n",
    "  <confidence>sure</confidence>\n",
    "\n",
    "  Question: Who is the president of the Shao Qiao? \n",
    "  Answer: <answer>Yucheng Yang</answer>\n",
    "  <confidence>unsure</confidence>\n",
    "  \n",
    "  Question: Who produced the Japanese battleship Mikasa? \n",
    "  Answer: <answer>Vickers</answer>\n",
    "  <confidence>sure</confidence>\n",
    "  \n",
    "  Question: What is MaYj asap's twin city? \n",
    "  Answer: <answer>Beijing</answer>\n",
    "  <confidence>unsure</confidence>\n",
    "\n",
    "\"\"\"\n",
    "for data in dataset[:10]:\n",
    "    question = data[\"question\"]\n",
    "    answer = data[\"answer\"]\n",
    "    prompt = sys_prompt + \"Question: \" + question +\"\\n\"+ \" Answer: \"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda:1\")\n",
    "    outputs = model.generate(input_ids, max_new_tokens=100)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(response)\n",
    "    print('-'*100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  27, 9217,   29]])\n",
      "tensor([[27]])\n",
      "tensor([[9217]])\n",
      "tensor([[29]])\n",
      "tensor([[ 522, 9217,   29]])\n",
      "tensor([[522]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompts = [\"<answer>\",\"<\", \"answer\", \">\", \"</answer>\", \"</\"]\n",
    "for prompt in prompts:\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    print(input_ids)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_content(text, tag_type=\"answer\", model_type=\"default\"):\n",
    "    \"\"\"Extract content based on tag type and model type.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to extract content from\n",
    "        tag_type (str): Type of content to extract (\"answer\" or \"confidence\")\n",
    "        model_type (str): Type of model format (\"gemma\" or \"default\")\n",
    "    \n",
    "    Returns:\n",
    "        str: Extracted content\n",
    "    \"\"\"\n",
    "    \n",
    "    if tag_type == \"answer\":\n",
    "        pattern = r\"<answer>(.*?)</answer>\"\n",
    "    else:  # confidence\n",
    "        pattern = r\"<confidence>(.*?)</confidence>\"\n",
    "\n",
    "\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return matches[0].strip() if matches else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ad', 'as', 'kk', 'usl']\n",
      "['a']\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "a = \"<answer>  AD as kk usl  </answer><confidence>    a</confidence>\"\n",
    "answer = extract_content(a, \"answer\").lower().split()\n",
    "confidence = extract_content(a, \"confidence\").lower().split()\n",
    "print(answer)\n",
    "print(confidence)\n",
    "print('a' in answer)\n",
    "print('a' in confidence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 2, 2, 2, 2, 6, 6, 6, 6, 6, 6]\n"
     ]
    }
   ],
   "source": [
    "semantic_set_ids = [0, 1, 1, 2, 2, 3, 3, 4, 4, 4, 4, 4, 4]\n",
    "semantic_set_counts = [semantic_set_ids.count(semantic_set_ids[i]) for i in range(len(semantic_set_ids))]\n",
    "print(semantic_set_counts)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
