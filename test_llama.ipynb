{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ipdb\n",
    "from vllm import LLM, SamplingParams\n",
    "import json\n",
    "import os\n",
    "model_name = \"/data/ah/code/rl/open-rs/results/pararel\"\n",
    "model_name = \"/data/ah/code/rl/open-rs/results/qwen1.5b_pararel_origin_reward_fix_unreward/checkpoint-600\"\n",
    "model_name = \"/data/ah/code/rl/open-rs/results/llama3/pararel_batch10_51_2\"\n",
    "model_name = \"/data/ah/code/rl/open-rs/results/llama3/pararel_batch10\"\n",
    "# cuda :1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "model = LLM(\n",
    "    model=model_name,\n",
    ")\n",
    "print(\"model load sucess\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.open_r1.rewards_u import extract_content, are_equivalent\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "\n",
    "# 定义采样参数\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.1,\n",
    "    max_tokens=40\n",
    ")\n",
    "\n",
    "def extract_content(text, tag_type=\"answer\"):\n",
    "    if tag_type == \"answer\":\n",
    "        pattern = r\"<answer>(.*?)</answer>\"\n",
    "    else:  # confidence\n",
    "        pattern = r\"<confidence>(.*?)</confidence>\"\n",
    "        # confidence = text.split(\"<confidence>\")[-1]\n",
    "        # confidence = confidence.split(\"</confidence>\")[0]\n",
    "        # return confidence.strip()\n",
    "\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return matches[0].strip() if matches else \"\"\n",
    "\n",
    "data_path = \"dataset/ID_test_pararel.json\"\n",
    "# data_path = \"dataset/OOD_test_pararel.json\"\n",
    "# data_path = \"dataset/pararel_training.json\"\n",
    "dataset = json.load(open(data_path))\n",
    "\n",
    "def inference(input_text):\n",
    "    outputs = model.generate(\n",
    "            input_text,\n",
    "            sampling_params=sampling_params\n",
    "    )\n",
    "    \n",
    "    output_text = outputs[0].outputs[0].text\n",
    "    # 找到第一个</confidence>， 保留第一个</confidence>及其之前的文本\n",
    "    result = output_text.split('</confidence>')[0] + '</confidence>'\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "sys_prompt = \"\"\"\n",
    "  You are a faithful assistant. The user asks a question, and you provide the answer and your confidence expressed in sure or unsure.\n",
    "  Question: Who is representing The Stranglers? \n",
    "  Answer: <answer>EMI</answer>\n",
    "  <confidence>sure</confidence>\n",
    "\n",
    "  Question: Who is the president of the Shao Qiao? \n",
    "  Answer: <answer>Yucheng Yang</answer>\n",
    "  <confidence>unsure</confidence>\n",
    "  \n",
    "  Question: Who produced the Japanese battleship Mikasa? \n",
    "  Answer: <answer>Vickers</answer>\n",
    "  <confidence>sure</confidence>\n",
    "  \n",
    "  Question: What is MaYj asap's twin city? \n",
    "  Answer: <answer>Beijing</answer>\n",
    "  <confidence>unsure</confidence>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "rft_pararel_results = []\n",
    "total_sample = len(dataset)\n",
    "correct_sample = 0\n",
    "un_c_match_sample = 0\n",
    "Q, I, C = 0, 0, 0\n",
    "for data in tqdm(dataset):\n",
    "    question = data[\"question\"]\n",
    "    answer = data[\"answer\"]\n",
    "    prompt = sys_prompt + \"Question: \" + question +\"\\n\"+ \" Answer: \"\n",
    "    output_text = inference(prompt)\n",
    "    model_ans = extract_content(output_text, \"answer\")\n",
    "    model_confidence = extract_content(output_text, \"confidence\")\n",
    "    # lines = output_text.split('\\n')\n",
    "    # if len(lines) == 0:\n",
    "    #     lines.append(\"\")\n",
    "    # if len(lines) == 1:\n",
    "    #     lines.append(\"\")\n",
    "    # model_ans = lines[0].removeprefix('<answer>').removesuffix('</answer>')\n",
    "    # model_confidence = lines[1]\n",
    "    # strict_eq = are_equivalent(answer, model_ans, strict_entailment=True, example=None)\n",
    "    # correct = 1 if answer.lower() in model_ans.lower() or strict_eq else 0\n",
    "    correct = 1 if answer.lower() in model_ans.lower() else 0\n",
    "    unsure = 1 if \"unsure\" in model_confidence.lower() else 0\n",
    "    un_c_match = 1 if (\"unsure\" in model_confidence.lower() and correct == 0 ) or (\"sure\" in model_confidence.lower() and correct == 1) else 0\n",
    "    # ipdb.set_trace()\n",
    "    CC, II = 0, 0\n",
    "    if correct == 1 and unsure == 0:\n",
    "        C += 1\n",
    "        CC = 1\n",
    "    if correct == 0 and unsure == 0:\n",
    "        I += 1\n",
    "        II = 1\n",
    "    # ipdb.set_trace()\n",
    "    rft_pararel_results.append({\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"model_ans\": model_ans,\n",
    "        \"model_confidence\": model_confidence,\n",
    "        # \"strict_eq\": strict_eq,\n",
    "        \"correct\": correct,\n",
    "        \"unsure\": unsure,\n",
    "        \"un_c_match\": un_c_match,\n",
    "        \"output_text\": output_text,\n",
    "        \"I\": II,\n",
    "        \"C\": CC,\n",
    "    })\n",
    "    correct_sample += correct\n",
    "    un_c_match_sample += un_c_match\n",
    "D = total_sample\n",
    "AED = math.sqrt(   (I * I + (D - C) * (D - C)) / (2 * D * D)   )\n",
    "\n",
    "print(f\"total_sample: {total_sample}\")\n",
    "print(f\"correct_sample: {correct_sample}\")\n",
    "print(f\"un_c_match_sample: {un_c_match_sample}\")\n",
    "print(f\"accuracy: {correct_sample/total_sample}\")\n",
    "print(f\"un_c_match_accuracy: {un_c_match_sample/total_sample}\")\n",
    "print(f\"AED: {AED}\")\n",
    "rft_pararel_results.append({\n",
    "    \"total_sample\": total_sample,\n",
    "    \"correct_sample\": correct_sample,\n",
    "    \"accuracy\": correct_sample/total_sample,\n",
    "    \"un_c_match_sample\": un_c_match_sample,\n",
    "    \"un_c_match_accuracy\": un_c_match_sample/total_sample,\n",
    "    \"AED\": AED,\n",
    "})\n",
    "# save results\n",
    "os.makedirs(\"results/eval_pararel/llama3/pararel_batch10_51_2\", exist_ok=True)\n",
    "json.dump(rft_pararel_results, open(\"results/eval_pararel/llama3/pararel_batch10_51_2/ID_pararel.json\", \"w\"), indent=4)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data_path = \"dataset/ID_test_pararel.json\"\n",
    "data_path = \"dataset/pararel_training.json\"\n",
    "dataset = json.load(open(data_path))\n",
    "  # You are a faithful assistant. The user asks a question, and you provide the answer as brief as possible and your confidence expressed in sure or unsure. \n",
    "\n",
    "sys_prompt = \"\"\"\n",
    "  You are a faithful assistant. The user asks a question, and you provide the answer and your confidence expressed in sure or unsure.\n",
    "  Question: Who is representing The Stranglers? \n",
    "  Answer: <answer>EMI</answer>\n",
    "  <confidence>sure</confidence>\n",
    "\n",
    "  Question: Who is the president of the Shao Qiao? \n",
    "  Answer: <answer>Yucheng Yang</answer>\n",
    "  <confidence>unsure</confidence>\n",
    "  \n",
    "  Question: Who produced the Japanese battleship Mikasa? \n",
    "  Answer: <answer>Vickers</answer>\n",
    "  <confidence>sure</confidence>\n",
    "  \n",
    "  Question: What is MaYj asap's twin city? \n",
    "  Answer: <answer>Beijing</answer>\n",
    "  <confidence>unsure</confidence>\n",
    "\n",
    "\"\"\"\n",
    "for data in dataset[:10]:\n",
    "    question = data[\"question\"]\n",
    "    answer = data[\"answer\"]\n",
    "    prompt = sys_prompt + \"Question: \" + question +\"\\n\"+ \" Answer: \"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda:1\")\n",
    "    outputs = model.generate(input_ids, max_new_tokens=100)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(response)\n",
    "    print('-'*100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompts = [\"<answer>\",\"<\", \"answer\", \">\", \"</answer>\", \"</\"]\n",
    "for prompt in prompts:\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    print(input_ids)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_content(text, tag_type=\"answer\", model_type=\"default\"):\n",
    "    \"\"\"Extract content based on tag type and model type.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to extract content from\n",
    "        tag_type (str): Type of content to extract (\"answer\" or \"confidence\")\n",
    "        model_type (str): Type of model format (\"gemma\" or \"default\")\n",
    "    \n",
    "    Returns:\n",
    "        str: Extracted content\n",
    "    \"\"\"\n",
    "    \n",
    "    if tag_type == \"answer\":\n",
    "        pattern = r\"<answer>(.*?)</answer>\"\n",
    "    else:  # confidence\n",
    "        pattern = r\"<confidence>(.*?)</confidence>\"\n",
    "\n",
    "\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return matches[0].strip() if matches else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"<answer>  AD as kk usl  </answer><confidence>    a</confidence>\"\n",
    "answer = extract_content(a, \"answer\").lower().split()\n",
    "confidence = extract_content(a, \"confidence\").lower().split()\n",
    "print(answer)\n",
    "print(confidence)\n",
    "print('a' in answer)\n",
    "print('a' in confidence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_set_ids = [0, 1, 1, 2, 2, 3, 3, 4, 4, 4, 4, 4, 4]\n",
    "semantic_set_counts = [semantic_set_ids.count(semantic_set_ids[i]) for i in range(len(semantic_set_ids))]\n",
    "print(semantic_set_counts)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
